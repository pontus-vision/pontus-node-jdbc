





import sys
print (sys.version)


spark.version








data = spark.range("0", "5")

(data
  .write
  .mode('overwrite')
  .format("delta")
  .save("/data/delta-table")
)


spark.sql("""
    SELECT * FROM delta.`/data/delta-test-2`""").toPandas()


spark.sql("""
CREATE TABLE IF NOT EXISTS auth_users (id INT, username STRING, password STRING) USING DELTA LOCATION '/data/delta-test-2'
""")



spark.sql("""
INSERT INTO auth_users (id, username, password) values (1, 'admin', '$2b$10$HS5fATEv6T2xxu8Osrmog.FFllVEzAvFdA.E2aUsp6R.EJsiRvexO')
""")


spark.sql("""
    SELECT * FROM delta.`/data/pv/auth_groups`
""").toPandas()


spark.sql("""

""").toPandas()





import pyspark.sql.functions as F
df = (spark
        .read
        .format("delta")
        .load("/data/delta-table")
        .orderBy("id")
      )

df = df.withColumn('foo', F.lit('2023-09-26T09:27:53.700Z'))
df = df.withColumn('bar', F.expr('month(foo) * 1000'))
df = df.addColumn('id', dataType='BIGINT', generatedAlwaysAs='xxhash64(foo)')
df.show()
df.schema['foo'].simpleString().split(':')[1]
display (df)



(df
  .write
  .mode('overwrite')
  .format("delta")
  .save("/data/delta-table")
)


spark.sql("""
SELECT *  from delta.`/data/delta-table` 
""").toPandas()








data = spark.range(5, 10)

(data
  .write
  .format("delta")
  .mode("overwrite")
  .save("/tmp/delta-table")
)





df = (spark
        .read
        .format("delta")
        .load("/tmp/delta-table")
        .orderBy("id")
      )

df.show()








from delta.tables import *
from pyspark.sql.functions import *

delta_table = DeltaTable.forPath(spark, "/tmp/delta-table")

# Update every even value by adding 100 to it
(delta_table
  .update(
    condition = expr("id % 2 == 0"),
    set = { "id": expr("id + 100") }
  )
)

(delta_table
  .toDF()
  .orderBy("id")
  .show()
)





# Delete every even value
(delta_table
  .delete(
    condition = expr("id % 2 == 0")
  )
)

(delta_table
  .toDF()
  .orderBy("id")
  .show()
)





# Upsert (merge) new data
new_data = spark.range(0, 20)

(delta_table.alias("old_data")
  .merge(
      new_data.alias("new_data"),
      "old_data.id = new_data.id"
      )
  .whenMatchedUpdate(set = { "id": col("new_data.id") })
  .whenNotMatchedInsert(values = { "id": col("new_data.id") })
  .execute()
)

(delta_table
  .toDF()
  .orderBy("id")
  .show()
)








# get the full history of the table
delta_table_history = (DeltaTable
                        .forPath(spark, "/tmp/delta-table")
                        .history()
                      )

(delta_table_history
   .select("version", "timestamp", "operation", "operationParameters", "operationMetrics", "engineInfo")
   .show()
)





# get the full history of the table
delta_table_history = (DeltaTable
                        .forPath(spark, "/tmp/delta-table")
                        .history()
                      )

(delta_table_history
   .select("version", "timestamp", "operation", "operationParameters", "operationMetrics", "engineInfo")
   .show()
)





df = (spark
        .read
        .format("delta")
        .load("/tmp/delta-table")
        .orderBy("id")
      )

df.show()





df = (spark
        .read
        .format("delta")
        .option("versionAsOf", 0) # we pass an option `versionAsOf` with the required version number we are interested in
        .load("/tmp/delta-table")
        .orderBy("id")
      )

df.show()





df = (spark
        .read
        .format("delta")
        .option("versionAsOf", 3) # we pass an option `versionAsOf` with the required version number we are interested in
        .load("/tmp/delta-table")
        .orderBy("id")
      )

df.show()





streaming_df = (spark
                 .readStream
                 .format("rate")
                 .load()
               )

stream = (streaming_df
            .selectExpr("value as id")
            .writeStream
            .format("delta")
            .option("checkpointLocation", "/tmp/checkpoint")
            .start("/tmp/delta-table")
          )


# To view the results of this step, view your container logs after execution using: docker logs --follow <first 4 number of container id>

stream2 = (spark
            .readStream
            .format("delta")
            .load("/tmp/delta-table")
            .writeStream
            .format("console")
            .start()
          )


import yaml

